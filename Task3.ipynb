{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A51155/CODSOFT/blob/main/Task3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMAGE CAPTIONING\n",
        "\n",
        "Combine computer vision and natural language processing to build an image captioning AI. Use pre-trained image recognition models like VGG or ResNet to extract features from images, and then use a recurrent neural network (RNN) or transformer-based model to generate captions for those images."
      ],
      "metadata": {
        "id": "A0kLCU1rETFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Add, Input\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# Load the pre-trained VGG16 model\n",
        "def extract_features(image_path):\n",
        "    model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
        "    image = load_img(image_path, target_size=(224, 224))\n",
        "    image = img_to_array(image)\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = tf.keras.applications.vgg16.preprocess_input(image)\n",
        "    features = model.predict(image)\n",
        "    return features\n",
        "\n",
        "# Prepare the dataset (images and captions)\n",
        "def load_dataset(images, captions):\n",
        "    features = []\n",
        "    for img in images:\n",
        "        features.append(extract_features(img))\n",
        "    return np.array(features), captions\n",
        "\n",
        "# Tokenize captions\n",
        "def tokenize_captions(captions):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(captions)\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    return tokenizer, vocab_size\n",
        "\n",
        "# Create the RNN model\n",
        "def create_model(vocab_size):\n",
        "    # Image feature input\n",
        "    image_input = Input(shape=(4096,))\n",
        "    image_features = Dense(256, activation='relu')(image_input)\n",
        "\n",
        "    # Caption input\n",
        "    caption_input = Input(shape=(None,))\n",
        "    caption_embedding = Embedding(vocab_size, 256)(caption_input)\n",
        "    caption_lstm = LSTM(256)(caption_embedding)\n",
        "\n",
        "    # Combine features\n",
        "    combined = Add()([image_features, caption_lstm])\n",
        "    output = Dense(vocab_size, activation='softmax')(combined)\n",
        "\n",
        "    model = Model(inputs=[image_input, caption_input], outputs=output)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "# Generate captions for a new image\n",
        "def generate_caption(model, tokenizer, image_path, max_length):\n",
        "    features = extract_features(image_path)\n",
        "    caption = [tokenizer.word_index['startseq']]\n",
        "    for _ in range(max_length):\n",
        "        sequence = pad_sequences([caption], maxlen=max_length)\n",
        "        prediction = model.predict([features, sequence])\n",
        "        word = np.argmax(prediction)\n",
        "        caption.append(word)\n",
        "        if word == tokenizer.word_index['endseq']:\n",
        "            break # This line was not indented correctly, causing the error.\n",
        "    return ' '.join([tokenizer.index_word[i] for i in caption if i in tokenizer.index_word])\n",
        "\n",
        "# Main function to run the image captioning\n",
        "def main():\n",
        "    # Example images and captions\n",
        "    # Assuming your images are in a folder\n",
        "    image_folder = 'path/to/your/images'\n",
        "    images = [os.path.join(image_folder, filename) for filename in os.listdir(image_folder)]\n",
        "    captions = ['caption for image 1', 'caption for image 2', ...]  # Replace with your captions\n",
        "\n",
        "\n",
        "    # Load and prepare the dataset\n",
        "    features, captions = load_dataset(images, captions)\n",
        "\n",
        "    # Tokenize the captions\n",
        "    tokenizer, vocab_size = tokenize_captions(captions)\n",
        "\n",
        "    # Create the model\n",
        "    model = create_model(vocab_size)\n",
        "\n",
        "    # Train the model\n",
        "    # ...\n",
        "\n",
        "    # Generate captions for new images\n",
        "    new_image_path = 'path/to/your/new/image.jpg'\n",
        "    max_length = 30\n"
      ],
      "metadata": {
        "id": "DJyb6cJZEz8_"
      },
      "execution_count": 7,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}